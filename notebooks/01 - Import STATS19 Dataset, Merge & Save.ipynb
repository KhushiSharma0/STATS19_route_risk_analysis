{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c462a5c",
   "metadata": {},
   "source": [
    "# 01 - Import STATS19 Dataset, Merge & Save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8ad806",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The UK Department for Transport (DfT) road casualty statistics consist of [three primary datasets](https://www.data.gov.uk/dataset/cb7ae6f0-4be6-4935-9277-47e5ce24a11f/road-accidents-safety-data):\n",
    "- Collision data: Information about each accident event\n",
    "- Vehicle data: Details about vehicles involved in accidents\n",
    "- Casualty data: Information about people injured in accidents\n",
    "\n",
    "These datasets are linked by common identifiers and can be quite large (approximately 4.5GB combined)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fb70a3",
   "metadata": {},
   "source": [
    "## Memory Challenges\n",
    "\n",
    "When processing large datasets, loading everything into memory at once can lead to `MemoryError` issues. A solution is to process the data in manageable chunks, focusing on:\n",
    "\n",
    "1. Reading data incrementally \n",
    "2. Applying early filtering to reduce data volume\n",
    "3. Only loading relevant portions of secondary datasets\n",
    "4. Writing results incrementally to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa8f9459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995f6f1e",
   "metadata": {},
   "source": [
    "### Chunked Processing Functions\n",
    "We define specialized functions to handle large data processing:\n",
    "- `load_filtered_csv`: loads and filters `.csv` files in chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04af1b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_filtered_csv(path, filter_func=None, dtype=None, chunksize=100_000):\n",
    "    \"\"\"\n",
    "    Load and filter a CSV file in chunks\n",
    "    \n",
    "    Parameters:\n",
    "    path : Path to CSV file\n",
    "    filter_func : Function to filter rows (optional)\n",
    "    dtype : Dictionary of column data types\n",
    "    chunksize : Number of rows to process at once\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for chunk in pd.read_csv(path, dtype=dtype, chunksize=chunksize, low_memory=False):\n",
    "        if filter_func is not None:\n",
    "            chunk = chunk[filter_func(chunk)]\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "    return pd.concat(chunks, ignore_index=True) if chunks else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8924fcee",
   "metadata": {},
   "source": [
    "- `filter_south_yorkshire`: filter function to isolate data for South Yorkshire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3db0c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_south_yorkshire(df):\n",
    "    return df['police_force'] == 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8793eb4e",
   "metadata": {},
   "source": [
    "`clean_and_organise_data`: removes redundant columns and orders identifier columns to the front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21efdf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_organize_data(merged_data):\n",
    "    \"\"\"\n",
    "    Clean up and organize columns in the merged dataset\n",
    "    \n",
    "    Parameters:\n",
    "    merged_data : DataFrame containing the merged data\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame with cleaned and reorganized columns\n",
    "    \"\"\"\n",
    "    print(\"  Cleaning and organizing columns...\")\n",
    "    \n",
    "    # List of columns to drop as identified earlier\n",
    "    columns_to_drop = [\n",
    "        'accident_year_x', 'accident_year_y',\n",
    "        'accident_reference_x', 'accident_reference_y'\n",
    "    ]\n",
    "    \n",
    "    # Drop redundant columns\n",
    "    cleaned_data = merged_data.drop(columns=columns_to_drop, errors='ignore')\n",
    "    \n",
    "    # Rename 'vehicle_reference_x' if it exists\n",
    "    if 'vehicle_reference_x' in cleaned_data.columns:\n",
    "        cleaned_data = cleaned_data.rename(columns={'vehicle_reference_x': 'vehicle_reference'})\n",
    "    \n",
    "    # Reorder columns to bring reference columns to the front\n",
    "    reference_columns = [col for col in [\n",
    "        'accident_index', 'accident_year', 'accident_reference', \n",
    "        'vehicle_reference', 'casualty_reference'\n",
    "    ] if col in cleaned_data.columns]\n",
    "    \n",
    "    # Identify remaining columns that aren't in reference_columns\n",
    "    remaining_columns = [col for col in cleaned_data.columns if col not in reference_columns]\n",
    "    \n",
    "    # Combine the lists to reorder DataFrame columns\n",
    "    ordered_columns = reference_columns + remaining_columns\n",
    "    \n",
    "    return cleaned_data[ordered_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe712a0d",
   "metadata": {},
   "source": [
    "- `process_in_chunks`: Main processing function that handles the entire workflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca99872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_in_chunks(casualty_path, collision_path, vehicle_path, output_path, \n",
    "                      filter_func=None, chunksize=50_000, dtype_dict=None):\n",
    "    \"\"\"\n",
    "    Process large datasets in chunks - reading, merging, cleaning and filtering\n",
    "    \n",
    "    Parameters:\n",
    "    casualty_path : Path to casualty CSV\n",
    "    collision_path : Path to collision CSV\n",
    "    vehicle_path : Path to vehicle CSV\n",
    "    output_path : Where to save the final filtered data\n",
    "    filter_func : Function to filter the final dataset (optional)\n",
    "    chunksize : Number of rows to process at once\n",
    "    dtype_dict : Dictionary of column data types\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Process collision data in chunks first (usually smaller than casualty data)\n",
    "    print(\"Loading and processing collision data...\")\n",
    "    collision_chunks = pd.read_csv(collision_path, dtype=dtype_dict, \n",
    "                                  chunksize=chunksize, low_memory=False)\n",
    "    \n",
    "    # Use the first chunk to initialize the output file\n",
    "    first_chunk = True\n",
    "    \n",
    "    for i, collision_chunk in enumerate(collision_chunks):\n",
    "        print(f\"Processing collision chunk {i+1}...\")\n",
    "        \n",
    "        # If we're filtering by police force, do it early to reduce data volume\n",
    "        if filter_func is not None:\n",
    "            collision_chunk = collision_chunk[filter_func(collision_chunk)]\n",
    "            \n",
    "            # Skip empty chunks\n",
    "            if len(collision_chunk) == 0:\n",
    "                continue\n",
    "        \n",
    "        # Get the accident indices in this chunk to filter other datasets\n",
    "        chunk_accident_indices = set(collision_chunk['accident_index'])\n",
    "        \n",
    "        # Load only the casualty data relevant to this chunk's accident indices\n",
    "        print(f\"  Loading relevant casualty data...\")\n",
    "        casualty_chunks = pd.read_csv(casualty_path, dtype=dtype_dict, \n",
    "                                     chunksize=chunksize, low_memory=False)\n",
    "        \n",
    "        relevant_casualty_data = []\n",
    "        for casualty_chunk in casualty_chunks:\n",
    "            filtered_casualty = casualty_chunk[\n",
    "                casualty_chunk['accident_index'].isin(chunk_accident_indices)]\n",
    "            \n",
    "            if not filtered_casualty.empty:\n",
    "                relevant_casualty_data.append(filtered_casualty)\n",
    "        \n",
    "        if not relevant_casualty_data:\n",
    "            continue\n",
    "            \n",
    "        casualty_data = pd.concat(relevant_casualty_data, ignore_index=True)\n",
    "        \n",
    "        # Merge casualty with this collision chunk\n",
    "        print(f\"  Merging casualty and collision data...\")\n",
    "        merged_casualty_collision = casualty_data.merge(\n",
    "            collision_chunk, on=\"accident_index\", how=\"inner\")\n",
    "        \n",
    "        # Get the accident indices and vehicle references in this merged chunk\n",
    "        merge_keys = merged_casualty_collision[['accident_index', 'vehicle_reference']]\n",
    "        \n",
    "        # Load only the vehicle data relevant to this chunk\n",
    "        print(f\"  Loading relevant vehicle data...\")\n",
    "        vehicle_chunks = pd.read_csv(vehicle_path, dtype=dtype_dict, \n",
    "                                   chunksize=chunksize, low_memory=False)\n",
    "        \n",
    "        relevant_vehicle_data = []\n",
    "        for vehicle_chunk in vehicle_chunks:\n",
    "            # Create a temporary key for matching\n",
    "            vehicle_chunk_keys = vehicle_chunk[['accident_index', 'vehicle_reference']]\n",
    "            # Merge the keys to identify matches\n",
    "            matches = pd.merge(vehicle_chunk_keys, merge_keys, \n",
    "                              on=['accident_index', 'vehicle_reference'],\n",
    "                              how='inner')\n",
    "            \n",
    "            if not matches.empty:\n",
    "                # Get the matched rows from vehicle chunk\n",
    "                matched_indices = matches.index\n",
    "                relevant_vehicle = vehicle_chunk.loc[matched_indices]\n",
    "                relevant_vehicle_data.append(relevant_vehicle)\n",
    "        \n",
    "        if not relevant_vehicle_data:\n",
    "            continue\n",
    "            \n",
    "        vehicle_data = pd.concat(relevant_vehicle_data, ignore_index=True)\n",
    "        \n",
    "        # Final merge for this chunk\n",
    "        print(f\"  Creating final merged dataset...\")\n",
    "        final_chunk = merged_casualty_collision.merge(\n",
    "            vehicle_data, on=[\"accident_index\", \"vehicle_reference\"], how=\"inner\")\n",
    "        \n",
    "        # Clean up the merged chunk using our separated function\n",
    "        final_chunk = clean_and_organize_data(final_chunk)\n",
    "        \n",
    "        # Write to output file\n",
    "        print(f\"  Writing processed data to file...\")\n",
    "        mode = 'w' if first_chunk else 'a'\n",
    "        header = first_chunk\n",
    "        final_chunk.to_csv(output_path, mode=mode, header=header, index=False)\n",
    "        first_chunk = False\n",
    "        \n",
    "        # Free memory\n",
    "        del collision_chunk, casualty_data, vehicle_data, final_chunk\n",
    "        \n",
    "    print(f\"Processing complete. Data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1350292",
   "metadata": {},
   "source": [
    "This function:\n",
    "\n",
    "1. Processes collision data in chunks\n",
    "2. For each chunk, applies filtering if specified\n",
    "3. Identifies accident indices in the current chunk\n",
    "4. Loads only relevant casualty and vehicle data using these indices\n",
    "5. Merges the datasets appropriately\n",
    "6. Cleans and organizes the columns\n",
    "7. Writes each processed chunk to the output file\n",
    "8. Frees memory after each chunk is processed\n",
    "\n",
    "#### Memory Optimization Strategy\n",
    "Our approach follows these key principles:\n",
    "\n",
    "1. Early filtering: Apply geographic filtering (South Yorkshire) early to minimize data volume\n",
    "2. Selective loading: Only load data relevant to the current processing chunk\n",
    "3. Incremental output: Write results to disk as they're processed rather than accumulating in memory\n",
    "4. Memory cleanup: Explicitly delete intermediate dataframes after they're no longer needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0222b1a4",
   "metadata": {},
   "source": [
    "#### Data Type Specification\n",
    "We specify data types for identifier columns to ensure consistent joining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac62630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify data types for critical columns\n",
    "dtype_dict = {\n",
    "    'accident_index': str,\n",
    "    'accident_year': str, \n",
    "    'accident_reference': str,\n",
    "    'vehicle_reference': str,\n",
    "    'casualty_reference': str\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbba429",
   "metadata": {},
   "source": [
    "Now that we have the functions defined, we can run the code and process our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f348bd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "my_dir_path = Path('F:/downloads')\n",
    "save_path = Path('../data/datasets')\n",
    "output_file = '../data/STATS19/dft_STATS19_1979_23_SY.csv'\n",
    "\n",
    "# Process the data in chunks\n",
    "process_in_chunks(\n",
    "    casualty_path=my_dir_path/'dft-road-casualty-statistics-casualty-1979-latest-published-year.csv',\n",
    "    collision_path=my_dir_path/'dft-road-casualty-statistics-collision-1979-latest-published-year.csv',\n",
    "    vehicle_path=my_dir_path/'dft-road-casualty-statistics-vehicle-1979-latest-published-year.csv',\n",
    "    output_path=output_file,\n",
    "    filter_func=filter_south_yorkshire,\n",
    "    chunksize=50_000,\n",
    "    dtype_dict=dtype_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328215ca",
   "metadata": {},
   "source": [
    "Now that we have imported the datasets, we can merge them to one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d3157f",
   "metadata": {},
   "source": [
    "There will be a few extra redundant columns formed due to joining the datasets, which we need to remove."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58af8321",
   "metadata": {},
   "source": [
    "Now, we can filter the whole dataset for only South Yorkshire data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
