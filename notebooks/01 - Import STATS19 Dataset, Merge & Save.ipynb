{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c462a5c",
   "metadata": {},
   "source": [
    "# 01 - Import STATS19 Dataset, Merge & Save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8ad806",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The UK Department for Transport (DfT) road casualty statistics consist of [three primary datasets](https://www.data.gov.uk/dataset/cb7ae6f0-4be6-4935-9277-47e5ce24a11f/road-accidents-safety-data):\n",
    "- Collision data: Information about each accident event\n",
    "- Vehicle data: Details about vehicles involved in accidents\n",
    "- Casualty data: Information about people injured in accidents\n",
    "\n",
    "These datasets are linked by common identifiers and can be quite large (approximately 4.5GB combined)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fb70a3",
   "metadata": {},
   "source": [
    "## Memory Challenges\n",
    "\n",
    "When processing large datasets, loading everything into memory at once can lead to `MemoryError` issues. A solution is to process the data in manageable chunks, focusing on:\n",
    "\n",
    "1. Reading data incrementally \n",
    "2. Applying early filtering to reduce data volume\n",
    "3. Only loading relevant portions of secondary datasets\n",
    "4. Writing results incrementally to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa8f9459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995f6f1e",
   "metadata": {},
   "source": [
    "### Chunked Processing Functions\n",
    "We define specialized functions to handle large data processing:\n",
    "- `load_filtered_csv`: loads and filters `.csv` files in chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04af1b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_filtered_csv(path, filter_func=None, dtype=None, chunksize=100_000):\n",
    "    \"\"\"\n",
    "    Load and filter a CSV file in chunks\n",
    "    \n",
    "    Parameters:\n",
    "    path : Path to CSV file\n",
    "    filter_func : Function to filter rows (optional)\n",
    "    dtype : Dictionary of column data types\n",
    "    chunksize : Number of rows to process at once\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for chunk in pd.read_csv(path, dtype=dtype, chunksize=chunksize, low_memory=False):\n",
    "        if filter_func is not None:\n",
    "            chunk = chunk[filter_func(chunk)]\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "    return pd.concat(chunks, ignore_index=True) if chunks else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8924fcee",
   "metadata": {},
   "source": [
    "- `filter_south_yorkshire`: filter function to isolate data for South Yorkshire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3db0c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_south_yorkshire(df):\n",
    "    return df['police_force'] == 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8793eb4e",
   "metadata": {},
   "source": [
    "`clean_and_organise_data`: removes redundant columns and orders identifier columns to the front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21efdf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_organize_data(merged_data):\n",
    "    \"\"\"\n",
    "    Clean up and organize columns in the merged dataset\n",
    "    \n",
    "    Parameters:\n",
    "    merged_data : DataFrame containing the merged data\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame with cleaned and reorganized columns\n",
    "    \"\"\"\n",
    "    print(\"  Cleaning and organizing columns...\")\n",
    "    \n",
    "    # List of columns to drop as identified earlier\n",
    "    columns_to_drop = [\n",
    "        'accident_year_x', 'accident_year_y',\n",
    "        'accident_reference_x', 'accident_reference_y'\n",
    "    ]\n",
    "    \n",
    "    # Drop redundant columns\n",
    "    cleaned_data = merged_data.drop(columns=columns_to_drop, errors='ignore')\n",
    "    \n",
    "    # Rename 'vehicle_reference_x' if it exists\n",
    "    if 'vehicle_reference_x' in cleaned_data.columns:\n",
    "        cleaned_data = cleaned_data.rename(columns={'vehicle_reference_x': 'vehicle_reference'})\n",
    "    \n",
    "    # Reorder columns to bring reference columns to the front\n",
    "    reference_columns = [col for col in [\n",
    "        'accident_index', 'accident_year', 'accident_reference', \n",
    "        'vehicle_reference', 'casualty_reference'\n",
    "    ] if col in cleaned_data.columns]\n",
    "    \n",
    "    # Identify remaining columns that aren't in reference_columns\n",
    "    remaining_columns = [col for col in cleaned_data.columns if col not in reference_columns]\n",
    "    \n",
    "    # Combine the lists to reorder DataFrame columns\n",
    "    ordered_columns = reference_columns + remaining_columns\n",
    "    \n",
    "    return cleaned_data[ordered_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe712a0d",
   "metadata": {},
   "source": [
    "- `process_in_chunks`: Main processing function that handles the entire workflow. Keeps relationship integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fca99872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_in_chunks(casualty_path, collision_path, vehicle_path, output_path, filter_func=None, chunksize=50_000, dtype_dict=None):\n",
    "    \"\"\"\n",
    "    Process large datasets while preserving relationships between records\n",
    "    \n",
    "    Parameters:\n",
    "    casualty_path : Path to casualty CSV\n",
    "    collision_path : Path to collision CSV\n",
    "    vehicle_path : Path to vehicle CSV\n",
    "    output_path : Where to save the final filtered data\n",
    "    filter_func : Function to filter the dataset (optional)\n",
    "    chunksize : Number of rows to process at once\n",
    "    dtype_dict : Dictionary of column data types\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Step 1: First filter collision data to get relevant accident indices\n",
    "    print(\"Step 1: Filtering collision data to get relevant accident indices...\")\n",
    "    relevant_accident_indices = set()\n",
    "    \n",
    "    for chunk in pd.read_csv(collision_path, dtype=dtype_dict, chunksize=chunksize, low_memory=False):\n",
    "        if filter_func is not None:\n",
    "            filtered_chunk = chunk[filter_func(chunk)]\n",
    "            if not filtered_chunk.empty:\n",
    "                relevant_accident_indices.update(filtered_chunk['accident_index'])\n",
    "    \n",
    "    print(f\"Found {len(relevant_accident_indices)} relevant accidents\")\n",
    "    \n",
    "    if not relevant_accident_indices:\n",
    "        print(\"No relevant data found after filtering. Process complete.\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Process each dataset in chunks, but filter by the complete set of accident indices\n",
    "    print(\"\\nStep 2: Processing casualty data...\")\n",
    "    processed_casualties = []\n",
    "    \n",
    "    for chunk in pd.read_csv(casualty_path, dtype=dtype_dict, chunksize=chunksize, low_memory=False):\n",
    "        filtered_chunk = chunk[chunk['accident_index'].isin(relevant_accident_indices)]\n",
    "        if not filtered_chunk.empty:\n",
    "            processed_casualties.append(filtered_chunk)\n",
    "    \n",
    "    if not processed_casualties:\n",
    "        print(\"No casualty data found for the filtered accidents. Process complete.\")\n",
    "        return\n",
    "    \n",
    "    casualty_data = pd.concat(processed_casualties, ignore_index=True)\n",
    "    print(f\"Processed {len(casualty_data)} casualty records\")\n",
    "    \n",
    "    # Step 3: Process vehicle data\n",
    "    print(\"\\nStep 3: Processing vehicle data...\")\n",
    "    processed_vehicles = []\n",
    "    \n",
    "    for chunk in pd.read_csv(vehicle_path, dtype=dtype_dict, chunksize=chunksize, low_memory=False):\n",
    "        filtered_chunk = chunk[chunk['accident_index'].isin(relevant_accident_indices)]\n",
    "        if not filtered_chunk.empty:\n",
    "            processed_vehicles.append(filtered_chunk)\n",
    "    \n",
    "    if not processed_vehicles:\n",
    "        print(\"No vehicle data found for the filtered accidents. Process complete.\")\n",
    "        return\n",
    "    \n",
    "    vehicle_data = pd.concat(processed_vehicles, ignore_index=True)\n",
    "    print(f\"Processed {len(vehicle_data)} vehicle records\")\n",
    "    \n",
    "    # Step 4: Load the filtered collision data\n",
    "    print(\"\\nStep 4: Loading filtered collision data...\")\n",
    "    processed_collisions = []\n",
    "    \n",
    "    for chunk in pd.read_csv(collision_path, dtype=dtype_dict, chunksize=chunksize, low_memory=False):\n",
    "        filtered_chunk = chunk[chunk['accident_index'].isin(relevant_accident_indices)]\n",
    "        if not filtered_chunk.empty:\n",
    "            processed_collisions.append(filtered_chunk)\n",
    "    \n",
    "    collision_data = pd.concat(processed_collisions, ignore_index=True)\n",
    "    print(f\"Loaded {len(collision_data)} collision records\")\n",
    "    \n",
    "    # Step 5: Merge datasets\n",
    "    print(\"\\nStep 5: Merging datasets...\")\n",
    "    print(\"  Merging casualty data with collision data...\")\n",
    "    merged_casualty_collision = casualty_data.merge(\n",
    "        collision_data, on=\"accident_index\", how=\"inner\")\n",
    "    \n",
    "    print(\"  Merging with vehicle data...\")\n",
    "    final_data = merged_casualty_collision.merge(\n",
    "        vehicle_data, on=[\"accident_index\", \"vehicle_reference\"], how=\"inner\")\n",
    "    \n",
    "    # Step 6: Clean and organize the merged data\n",
    "    print(\"\\nStep 6: Cleaning and organizing data...\")\n",
    "    final_data = clean_and_organize_data(final_data)\n",
    "    \n",
    "    # Step 7: Write the complete dataset\n",
    "    print(\"\\nStep 7: Writing processed data to file...\")\n",
    "    final_data.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"\\nProcessing complete. {len(final_data)} records saved to {output_path}\")\n",
    "    \n",
    "    # Clean up memory\n",
    "    del casualty_data, collision_data, vehicle_data, final_data, merged_casualty_collision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1350292",
   "metadata": {},
   "source": [
    "This function:\n",
    "\n",
    "1. Processes collision data in chunks\n",
    "2. For each chunk, applies filtering if specified\n",
    "3. Identifies accident indices in the current chunk\n",
    "4. Loads only relevant casualty and vehicle data using these indices\n",
    "5. Merges the datasets appropriately\n",
    "6. Cleans and organizes the columns\n",
    "7. Writes each processed chunk to the output file\n",
    "8. Frees memory after each chunk is processed\n",
    "\n",
    "#### Memory Optimization Strategy\n",
    "Our approach follows these key principles:\n",
    "\n",
    "1. Early filtering: Apply geographic filtering (South Yorkshire) early to minimize data volume\n",
    "2. Selective loading: Only load data relevant to the current processing chunk\n",
    "3. Incremental output: Write results to disk as they're processed rather than accumulating in memory\n",
    "4. Memory cleanup: Explicitly delete intermediate dataframes after they're no longer needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0222b1a4",
   "metadata": {},
   "source": [
    "#### Data Type Specification\n",
    "We specify data types for identifier columns to ensure consistent joining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ac62630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify data types for critical columns\n",
    "dtype_dict = {\n",
    "    'accident_index': str,\n",
    "    'accident_year': str, \n",
    "    'accident_reference': str,\n",
    "    'vehicle_reference': str,\n",
    "    'casualty_reference': str\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbba429",
   "metadata": {},
   "source": [
    "Now that we have the functions defined, we can run the code and process our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f348bd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Filtering collision data to get relevant accident indices...\n",
      "Found 180391 relevant accidents\n",
      "\n",
      "Step 2: Processing casualty data...\n",
      "Processed 243191 casualty records\n",
      "\n",
      "Step 3: Processing vehicle data...\n",
      "Processed 315043 vehicle records\n",
      "\n",
      "Step 4: Loading filtered collision data...\n",
      "Loaded 180391 collision records\n",
      "\n",
      "Step 5: Merging datasets...\n",
      "  Merging casualty data with collision data...\n",
      "  Merging with vehicle data...\n",
      "\n",
      "Step 6: Cleaning and organizing data...\n",
      "  Cleaning and organizing columns...\n",
      "\n",
      "Step 7: Writing processed data to file...\n",
      "\n",
      "Processing complete. 243191 records saved to ../data/STATS19/dft_STATS19_1979_23_SY.csv\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "my_dir_path = Path('F:/downloads')\n",
    "save_path = Path('../data/datasets')\n",
    "output_file = '../data/STATS19/dft_STATS19_1979_23_SY.csv'\n",
    "\n",
    "# Process the data in chunks\n",
    "process_in_chunks(\n",
    "    casualty_path=my_dir_path/'dft-road-casualty-statistics-casualty-1979-latest-published-year.csv',\n",
    "    collision_path=my_dir_path/'dft-road-casualty-statistics-collision-1979-latest-published-year.csv',\n",
    "    vehicle_path=my_dir_path/'dft-road-casualty-statistics-vehicle-1979-latest-published-year.csv',\n",
    "    output_path=output_file,\n",
    "    filter_func=filter_south_yorkshire,\n",
    "    chunksize=50_000,\n",
    "    dtype_dict=dtype_dict\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
